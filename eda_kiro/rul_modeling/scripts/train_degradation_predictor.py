"""
Task 3.2 & 3.3: åŠ£åŒ–äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰

Task 3.2: åŠ£åŒ–åº¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
- Random Forest Regressorã§ç¾åœ¨ã®ç‰¹å¾´é‡ã‹ã‚‰åŠ£åŒ–åº¦ã‚’äºˆæ¸¬
- Train/Val/Teståˆ†å‰²ï¼ˆã‚³ãƒ³ãƒ‡ãƒ³ã‚µãƒ™ãƒ¼ã‚¹ï¼‰

Task 3.3: æ¬¡ã‚µã‚¤ã‚¯ãƒ«å¿œç­”æ€§äºˆæ¸¬
- éå»Nã‚µã‚¤ã‚¯ãƒ«ã‹ã‚‰æ¬¡ã‚µã‚¤ã‚¯ãƒ«ã®å¿œç­”æ€§ç‰¹å¾´é‡ã‚’äºˆæ¸¬
- æ™‚ç³»åˆ—äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ï¼ˆRandom Forestï¼‰
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pickle

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Hiragino Sans']
plt.rcParams['axes.unicode_minus'] = False

def load_data():
    """åŠ£åŒ–åº¦ã‚¹ã‚³ã‚¢ä»˜ããƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    data_path = Path("output/degradation_prediction/features_with_degradation_score.csv")
    df = pd.read_csv(data_path)
    print(f"âœ“ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(df)}ã‚µãƒ³ãƒ—ãƒ«")
    return df

def split_data(df):
    """
    ã‚³ãƒ³ãƒ‡ãƒ³ã‚µãƒ™ãƒ¼ã‚¹ã§Train/Val/Teståˆ†å‰²
    Train: C1-C5 (1000ã‚µãƒ³ãƒ—ãƒ«)
    Val: C6 (200ã‚µãƒ³ãƒ—ãƒ«)
    Test: C7-C8 (400ã‚µãƒ³ãƒ—ãƒ«)
    """
    train_df = df[df['capacitor_id'].isin(['ES12C1', 'ES12C2', 'ES12C3', 'ES12C4', 'ES12C5'])].copy()
    val_df = df[df['capacitor_id'] == 'ES12C6'].copy()
    test_df = df[df['capacitor_id'].isin(['ES12C7', 'ES12C8'])].copy()
    
    print(f"\nâœ“ ãƒ‡ãƒ¼ã‚¿åˆ†å‰²:")
    print(f"  Train: {len(train_df)}ã‚µãƒ³ãƒ—ãƒ« (C1-C5)")
    print(f"  Val: {len(val_df)}ã‚µãƒ³ãƒ—ãƒ« (C6)")
    print(f"  Test: {len(test_df)}ã‚µãƒ³ãƒ—ãƒ« (C7-C8)")
    
    return train_df, val_df, test_df


def train_degradation_predictor(train_df, val_df):
    """
    Task 3.2: åŠ£åŒ–åº¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
    
    å…¥åŠ›: æ³¢å½¢ç‰¹æ€§ç‰¹å¾´é‡ï¼ˆ7å€‹ï¼‰
    å‡ºåŠ›: åŠ£åŒ–åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0-1ï¼‰
    """
    # ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ï¼ˆæ³¢å½¢ç‰¹æ€§ã®ã¿ã€ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ãªã—ï¼‰
    feature_cols = [
        'waveform_correlation',
        'vo_variability',
        'vl_variability',
        'response_delay',
        'response_delay_normalized',
        'residual_energy_ratio',
        'vo_complexity'
    ]
    
    target_col = 'degradation_score'
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿
    X_train = train_df[feature_cols].values
    y_train = train_df[target_col].values
    
    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
    X_val = val_df[feature_cols].values
    y_val = val_df[target_col].values
    
    # Random Forest Regressorã®å­¦ç¿’
    print("\n" + "="*60)
    print("Task 3.2: åŠ£åŒ–åº¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’")
    print("="*60)
    
    model = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    )
    
    model.fit(X_train, y_train)
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡
    y_train_pred = model.predict(X_train)
    train_mae = mean_absolute_error(y_train, y_train_pred)
    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
    train_r2 = r2_score(y_train, y_train_pred)
    
    print(f"\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿æ€§èƒ½:")
    print(f"  MAE: {train_mae:.4f}")
    print(f"  RMSE: {train_rmse:.4f}")
    print(f"  RÂ²: {train_r2:.4f}")
    
    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡
    y_val_pred = model.predict(X_val)
    val_mae = mean_absolute_error(y_val, y_val_pred)
    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
    val_r2 = r2_score(y_val, y_val_pred)
    
    print(f"\næ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æ€§èƒ½:")
    print(f"  MAE: {val_mae:.4f}")
    print(f"  RMSE: {val_rmse:.4f}")
    print(f"  RÂ²: {val_r2:.4f}")
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"\nç‰¹å¾´é‡é‡è¦åº¦:")
    for _, row in feature_importance.iterrows():
        print(f"  {row['feature']}: {row['importance']:.4f}")
    
    return model, feature_cols, feature_importance


def create_sequence_data(df, feature_cols, lookback=5):
    """
    Task 3.3ç”¨: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
    
    éå»lookbackã‚µã‚¤ã‚¯ãƒ«ã®ç‰¹å¾´é‡ã‹ã‚‰æ¬¡ã‚µã‚¤ã‚¯ãƒ«ã®ç‰¹å¾´é‡ã‚’äºˆæ¸¬
    """
    sequences = []
    targets = []
    capacitor_ids = []
    cycles = []
    
    for cap_id in df['capacitor_id'].unique():
        cap_data = df[df['capacitor_id'] == cap_id].sort_values('cycle')
        
        for i in range(lookback, len(cap_data)):
            # éå»lookbackã‚µã‚¤ã‚¯ãƒ«ã®ç‰¹å¾´é‡
            seq = cap_data.iloc[i-lookback:i][feature_cols].values.flatten()
            # æ¬¡ã‚µã‚¤ã‚¯ãƒ«ã®ç‰¹å¾´é‡
            target = cap_data.iloc[i][feature_cols].values
            
            sequences.append(seq)
            targets.append(target)
            capacitor_ids.append(cap_id)
            cycles.append(cap_data.iloc[i]['cycle'])
    
    return np.array(sequences), np.array(targets), np.array(capacitor_ids), np.array(cycles)

def train_response_predictor(train_df, val_df):
    """
    Task 3.3: æ¬¡ã‚µã‚¤ã‚¯ãƒ«å¿œç­”æ€§äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
    
    å…¥åŠ›: éå»5ã‚µã‚¤ã‚¯ãƒ«ã®æ³¢å½¢ç‰¹æ€§ç‰¹å¾´é‡
    å‡ºåŠ›: æ¬¡ã‚µã‚¤ã‚¯ãƒ«ã®æ³¢å½¢ç‰¹æ€§ç‰¹å¾´é‡
    """
    print("\n" + "="*60)
    print("Task 3.3: æ¬¡ã‚µã‚¤ã‚¯ãƒ«å¿œç­”æ€§äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’")
    print("="*60)
    
    feature_cols = [
        'waveform_correlation',
        'vo_variability',
        'vl_variability',
        'response_delay',
        'response_delay_normalized',
        'residual_energy_ratio',
        'vo_complexity'
    ]
    
    lookback = 5
    
    # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
    X_train, y_train, _, _ = create_sequence_data(train_df, feature_cols, lookback)
    X_val, y_val, val_cap_ids, val_cycles = create_sequence_data(val_df, feature_cols, lookback)
    
    print(f"\næ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ä½œæˆ:")
    print(f"  Train: {len(X_train)}ã‚µãƒ³ãƒ—ãƒ«")
    print(f"  Val: {len(X_val)}ã‚µãƒ³ãƒ—ãƒ«")
    print(f"  å…¥åŠ›æ¬¡å…ƒ: {X_train.shape[1]} (éå»{lookback}ã‚µã‚¤ã‚¯ãƒ« Ã— {len(feature_cols)}ç‰¹å¾´é‡)")
    print(f"  å‡ºåŠ›æ¬¡å…ƒ: {y_train.shape[1]} ({len(feature_cols)}ç‰¹å¾´é‡)")
    
    # å„ç‰¹å¾´é‡ã”ã¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
    models = {}
    
    for i, feature_name in enumerate(feature_cols):
        print(f"\n{feature_name}ã®äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
        
        model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        model.fit(X_train, y_train[:, i])
        
        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡
        y_val_pred = model.predict(X_val)
        val_mae = mean_absolute_error(y_val[:, i], y_val_pred)
        val_rmse = np.sqrt(mean_squared_error(y_val[:, i], y_val_pred))
        val_r2 = r2_score(y_val[:, i], y_val_pred)
        
        print(f"  Val MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}, RÂ²: {val_r2:.4f}")
        
        models[feature_name] = model
    
    return models, feature_cols, lookback


def evaluate_on_test(degradation_model, response_models, test_df, feature_cols, lookback):
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡"""
    print("\n" + "="*60)
    print("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡")
    print("="*60)
    
    # Task 3.2: åŠ£åŒ–åº¦äºˆæ¸¬ã®è©•ä¾¡
    X_test = test_df[feature_cols].values
    y_test = test_df['degradation_score'].values
    
    y_test_pred = degradation_model.predict(X_test)
    test_mae = mean_absolute_error(y_test, y_test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    test_r2 = r2_score(y_test, y_test_pred)
    
    print(f"\nTask 3.2 - åŠ£åŒ–åº¦äºˆæ¸¬:")
    print(f"  MAE: {test_mae:.4f}")
    print(f"  RMSE: {test_rmse:.4f}")
    print(f"  RÂ²: {test_r2:.4f}")
    
    success = "âœ… æˆåŠŸ" if test_mae < 0.1 else "âš ï¸ ç›®æ¨™æœªé”"
    print(f"  æˆåŠŸåŸºæº– (MAE < 0.1): {success}")
    
    # Task 3.3: æ¬¡ã‚µã‚¤ã‚¯ãƒ«å¿œç­”æ€§äºˆæ¸¬ã®è©•ä¾¡
    X_test_seq, y_test_seq, test_cap_ids, test_cycles = create_sequence_data(
        test_df, feature_cols, lookback
    )
    
    print(f"\nTask 3.3 - æ¬¡ã‚µã‚¤ã‚¯ãƒ«å¿œç­”æ€§äºˆæ¸¬:")
    
    for i, feature_name in enumerate(feature_cols):
        model = response_models[feature_name]
        y_pred = model.predict(X_test_seq)
        
        mae = mean_absolute_error(y_test_seq[:, i], y_pred)
        rmse = np.sqrt(mean_squared_error(y_test_seq[:, i], y_pred))
        r2 = r2_score(y_test_seq[:, i], y_pred)
        
        print(f"  {feature_name}:")
        print(f"    MAE: {mae:.4f}, RMSE: {rmse:.4f}, RÂ²: {r2:.4f}")
    
    return {
        'degradation': {
            'y_true': y_test,
            'y_pred': y_test_pred,
            'mae': test_mae,
            'rmse': test_rmse,
            'r2': test_r2
        },
        'response': {
            'X': X_test_seq,
            'y_true': y_test_seq,
            'cap_ids': test_cap_ids,
            'cycles': test_cycles,
            'feature_cols': feature_cols
        }
    }


def visualize_results(test_results, test_df, response_models, feature_importance):
    """çµæœã®å¯è¦–åŒ–"""
    output_dir = Path("output/degradation_prediction")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    fig = plt.figure(figsize=(20, 16))
    gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)
    
    # 1. åŠ£åŒ–åº¦äºˆæ¸¬: çœŸå€¤ vs äºˆæ¸¬å€¤
    ax1 = fig.add_subplot(gs[0, 0])
    y_true = test_results['degradation']['y_true']
    y_pred = test_results['degradation']['y_pred']
    
    ax1.scatter(y_true, y_pred, alpha=0.5, s=20)
    ax1.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect Prediction')
    ax1.set_xlabel('True Degradation Score', fontsize=12)
    ax1.set_ylabel('Predicted Degradation Score', fontsize=12)
    ax1.set_title('åŠ£åŒ–åº¦äºˆæ¸¬: çœŸå€¤ vs äºˆæ¸¬å€¤', fontsize=13, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. åŠ£åŒ–åº¦äºˆæ¸¬: æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆï¼ˆã‚³ãƒ³ãƒ‡ãƒ³ã‚µåˆ¥ï¼‰
    ax2 = fig.add_subplot(gs[0, 1])
    test_df_with_pred = test_df.copy()
    test_df_with_pred['degradation_pred'] = y_pred
    
    for cap_id in sorted(test_df['capacitor_id'].unique()):
        cap_data = test_df_with_pred[test_df_with_pred['capacitor_id'] == cap_id].sort_values('cycle')
        ax2.plot(cap_data['cycle'], cap_data['degradation_score'], 
                label=f'C{cap_id} True', linestyle='-', linewidth=2)
        ax2.plot(cap_data['cycle'], cap_data['degradation_pred'], 
                label=f'C{cap_id} Pred', linestyle='--', linewidth=2, alpha=0.7)
    
    ax2.set_xlabel('Cycle', fontsize=12)
    ax2.set_ylabel('Degradation Score', fontsize=12)
    ax2.set_title('åŠ£åŒ–åº¦äºˆæ¸¬: æ™‚ç³»åˆ—ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰', fontsize=13, fontweight='bold')
    ax2.legend(fontsize=8, ncol=2)
    ax2.grid(True, alpha=0.3)
    
    # 3. ç‰¹å¾´é‡é‡è¦åº¦
    ax3 = fig.add_subplot(gs[0, 2])
    ax3.barh(feature_importance['feature'], feature_importance['importance'])
    ax3.set_xlabel('Importance', fontsize=12)
    ax3.set_title('ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆåŠ£åŒ–åº¦äºˆæ¸¬ï¼‰', fontsize=13, fontweight='bold')
    ax3.grid(True, alpha=0.3, axis='x')
    
    # 4-9. æ¬¡ã‚µã‚¤ã‚¯ãƒ«å¿œç­”æ€§äºˆæ¸¬ï¼ˆå„ç‰¹å¾´é‡ï¼‰
    response_data = test_results['response']
    feature_cols = response_data['feature_cols']
    
    for idx, feature_name in enumerate(feature_cols):
        ax = fig.add_subplot(gs[1 + idx // 3, idx % 3])
        
        model = response_models[feature_name]
        y_pred = model.predict(response_data['X'])
        y_true = response_data['y_true'][:, idx]
        
        ax.scatter(y_true, y_pred, alpha=0.5, s=20)
        
        # Perfect prediction line
        min_val = min(y_true.min(), y_pred.min())
        max_val = max(y_true.max(), y_pred.max())
        ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)
        
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        
        ax.set_xlabel('True Value', fontsize=10)
        ax.set_ylabel('Predicted Value', fontsize=10)
        ax.set_title(f'{feature_name}\nMAE: {mae:.4f}, RÂ²: {r2:.4f}', 
                    fontsize=11, fontweight='bold')
        ax.grid(True, alpha=0.3)
    
    plt.suptitle('Phase 3: åŠ£åŒ–äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡çµæœ', fontsize=16, fontweight='bold', y=0.995)
    
    # ä¿å­˜
    output_path = output_dir / "prediction_model_evaluation.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nâœ“ å¯è¦–åŒ–ä¿å­˜: {output_path}")
    plt.close()


def save_models(degradation_model, response_models, feature_cols, lookback, feature_importance):
    """ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
    output_dir = Path("output/models_v3")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Task 3.2: åŠ£åŒ–åº¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
    model_path = output_dir / "degradation_predictor.pkl"
    with open(model_path, 'wb') as f:
        pickle.dump(degradation_model, f)
    print(f"\nâœ“ åŠ£åŒ–åº¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_path}")
    
    # ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ
    features_path = output_dir / "degradation_predictor_features.txt"
    with open(features_path, 'w') as f:
        for col in feature_cols:
            f.write(f"{col}\n")
    print(f"âœ“ ç‰¹å¾´é‡ãƒªã‚¹ãƒˆä¿å­˜: {features_path}")
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    importance_path = output_dir / "degradation_predictor_feature_importance.csv"
    feature_importance.to_csv(importance_path, index=False)
    print(f"âœ“ ç‰¹å¾´é‡é‡è¦åº¦ä¿å­˜: {importance_path}")
    
    # Task 3.3: æ¬¡ã‚µã‚¤ã‚¯ãƒ«å¿œç­”æ€§äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
    response_model_path = output_dir / "response_predictor.pkl"
    with open(response_model_path, 'wb') as f:
        pickle.dump({
            'models': response_models,
            'feature_cols': feature_cols,
            'lookback': lookback
        }, f)
    print(f"âœ“ æ¬¡ã‚µã‚¤ã‚¯ãƒ«å¿œç­”æ€§äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {response_model_path}")

def create_summary_document(test_results, feature_importance):
    """å®Œäº†ã‚µãƒãƒªãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä½œæˆ"""
    output_dir = Path("output/degradation_prediction")
    doc_path = output_dir / "phase3_completion_summary.md"
    
    with open(doc_path, 'w', encoding='utf-8') as f:
        f.write("# Phase 3 å®Œäº†ã‚µãƒãƒªãƒ¼: åŠ£åŒ–äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰\n\n")
        f.write("**å®Œäº†æ—¥**: 2026-01-18\n")
        f.write("**Phase**: Phase 3 - åŠ£åŒ–äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰\n\n")
        f.write("---\n\n")
        
        f.write("## ğŸ¯ Phase 3ã®ç›®çš„\n\n")
        f.write("å¿œç­”æ€§ã®åŠ£åŒ–åº¦ã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã€‚\n\n")
        
        f.write("---\n\n")
        f.write("## ğŸ“‹ å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯\n\n")
        
        # Task 3.1
        f.write("### Task 3.1: åŠ£åŒ–åº¦ã®å®šç¾© âœ…\n\n")
        f.write("**å®Ÿè£…å†…å®¹**:\n")
        f.write("- è¤‡åˆæŒ‡æ¨™ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: 4ã¤ã®æ³¢å½¢ç‰¹æ€§ã‚’çµ„ã¿åˆã‚ã›\n")
        f.write("- åŠ£åŒ–åº¦ã‚¹ã‚³ã‚¢ç¯„å›²: 0.000 - 0.731\n")
        f.write("- åŠ£åŒ–ã‚¹ãƒ†ãƒ¼ã‚¸å®šç¾©: Normal, Degrading, Severe, Critical\n\n")
        
        # Task 3.2
        f.write("### Task 3.2: åŠ£åŒ–åº¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ âœ…\n\n")
        f.write("**ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**: Random Forest Regressor\n\n")
        f.write("**ä½¿ç”¨ç‰¹å¾´é‡** (7å€‹ã®æ³¢å½¢ç‰¹æ€§):\n")
        for _, row in feature_importance.iterrows():
            f.write(f"- {row['feature']}: {row['importance']:.4f}\n")
        f.write("\n")
        
        deg_results = test_results['degradation']
        f.write("**ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ€§èƒ½**:\n")
        f.write(f"- MAE: {deg_results['mae']:.4f}\n")
        f.write(f"- RMSE: {deg_results['rmse']:.4f}\n")
        f.write(f"- RÂ²: {deg_results['r2']:.4f}\n\n")
        
        success = "âœ… é”æˆ" if deg_results['mae'] < 0.1 else "âš ï¸ æœªé”æˆ"
        f.write(f"**æˆåŠŸåŸºæº– (MAE < 0.1)**: {success}\n\n")
        
        # Task 3.3
        f.write("### Task 3.3: æ¬¡ã‚µã‚¤ã‚¯ãƒ«å¿œç­”æ€§äºˆæ¸¬ âœ…\n\n")
        f.write("**ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**: Random Forest Regressor (ç‰¹å¾´é‡ã”ã¨)\n\n")
        f.write("**å…¥åŠ›**: éå»5ã‚µã‚¤ã‚¯ãƒ«ã®æ³¢å½¢ç‰¹æ€§ç‰¹å¾´é‡\n")
        f.write("**å‡ºåŠ›**: æ¬¡ã‚µã‚¤ã‚¯ãƒ«ã®æ³¢å½¢ç‰¹æ€§ç‰¹å¾´é‡\n\n")
        
        f.write("---\n\n")
        f.write("## ğŸ“Š Phase 3ã®æˆæœ\n\n")
        f.write("### æ§‹ç¯‰ã—ãŸãƒ¢ãƒ‡ãƒ«\n\n")
        f.write("1. **åŠ£åŒ–åº¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«**: ç¾åœ¨ã®æ³¢å½¢ç‰¹æ€§ã‹ã‚‰åŠ£åŒ–åº¦ã‚’äºˆæ¸¬\n")
        f.write("2. **æ¬¡ã‚µã‚¤ã‚¯ãƒ«å¿œç­”æ€§äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«**: éå»5ã‚µã‚¤ã‚¯ãƒ«ã‹ã‚‰æ¬¡ã‚µã‚¤ã‚¯ãƒ«ã®æ³¢å½¢ç‰¹æ€§ã‚’äºˆæ¸¬\n\n")
        
        f.write("### å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«\n\n")
        f.write("**ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«**:\n")
        f.write("- `output/models_v3/degradation_predictor.pkl`\n")
        f.write("- `output/models_v3/response_predictor.pkl`\n")
        f.write("- `output/models_v3/degradation_predictor_features.txt`\n")
        f.write("- `output/models_v3/degradation_predictor_feature_importance.csv`\n\n")
        
        f.write("**çµæœãƒ•ã‚¡ã‚¤ãƒ«**:\n")
        f.write("- `output/degradation_prediction/prediction_model_evaluation.png`\n")
        f.write("- `output/degradation_prediction/phase3_completion_summary.md`\n\n")
        
        f.write("---\n\n")
        f.write("## ğŸ‰ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Œäº†\n\n")
        f.write("Phase 1, 2, 3ã®å…¨ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n\n")
        f.write("**Phase 1**: VL-VOé–¢ä¿‚æ€§åˆ†æ âœ…\n")
        f.write("**Phase 2**: ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ âœ…\n")
        f.write("**Phase 3**: åŠ£åŒ–äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ âœ…\n\n")
    
    print(f"âœ“ å®Œäº†ã‚µãƒãƒªãƒ¼ä¿å­˜: {doc_path}")

def main():
    print("="*60)
    print("Task 3.2 & 3.3: åŠ£åŒ–äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰")
    print("="*60)
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_data()
    
    # 2. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    train_df, val_df, test_df = split_data(df)
    
    # 3. Task 3.2: åŠ£åŒ–åº¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
    degradation_model, feature_cols, feature_importance = train_degradation_predictor(
        train_df, val_df
    )
    
    # 4. Task 3.3: æ¬¡ã‚µã‚¤ã‚¯ãƒ«å¿œç­”æ€§äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
    response_models, _, lookback = train_response_predictor(train_df, val_df)
    
    # 5. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡
    test_results = evaluate_on_test(
        degradation_model, response_models, test_df, feature_cols, lookback
    )
    
    # 6. å¯è¦–åŒ–
    visualize_results(test_results, test_df, response_models, feature_importance)
    
    # 7. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    save_models(degradation_model, response_models, feature_cols, lookback, feature_importance)
    
    # 8. å®Œäº†ã‚µãƒãƒªãƒ¼ã®ä½œæˆ
    create_summary_document(test_results, feature_importance)
    
    print("\n" + "="*60)
    print("âœ“ Phase 3å®Œäº†: åŠ£åŒ–äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰")
    print("="*60)
    print("\nğŸ‰ å…¨Phaseå®Œäº†ï¼")
    print("  Phase 1: VL-VOé–¢ä¿‚æ€§åˆ†æ âœ…")
    print("  Phase 2: ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ âœ…")
    print("  Phase 3: åŠ£åŒ–äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ âœ…")

if __name__ == "__main__":
    main()
